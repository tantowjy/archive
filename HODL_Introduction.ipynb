{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FPXC6lGgqhJ1"
   },
   "source": [
    "# Hand-On Deep Learing - Introduction Session\n",
    "\n",
    "This session will introduce you to the basic concepts of differentiable programming and traning neural networks from scratch. We will be using the popular deep learning framework PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1678897494401,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "E2y1UqizqeUr"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TM0WBd5RpGGa"
   },
   "source": [
    "We will first talk about the bread-and-butter data type of deep learning - tensors. Once done, we will do a quick introduction to differentiable programs. On a simple example, we will show how to compute gradients in pytorch, and then lead you towards an implementation of a naive gradient descent training loop.\n",
    "\n",
    "Having got a better feel of how one trains the parameters of a differentiable program, we will introduce neural networks. We will provide you with a full implementation of a training loop, and help you train an approximation of a Boolean function.\n",
    "\n",
    "We will then steer away from illustrative examples and get started with practical, application-oriented deep learning. We will train both shallow and deep neural networks for image classification, tune the parameters of training and the sizes of architectures, and observe how the individual properties of our training setup influence the quality of the learning outcomes.\n",
    "\n",
    "We will conclude this session with the introduction of convolutional layers. The challenge of the day will be to use convolutional neural networks to correctly classify greyscale images of fashion articles. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kWg7Abwlot8z"
   },
   "source": [
    "## Prelude: Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SsArlqzXFU1X"
   },
   "source": [
    "For training of deep models, `torch` uses a special data type: `torch.tensor`. `torch.tensor` is an encapsulation of uniform nested lists that allows for some of these lists to be *trainable*, meaning that their values can be figured out in training on data.\n",
    "\n",
    "Before we can move on to do anything more exciting, one has to know a bit about tensors. All you need to know is that...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ggXYIWqrF5gi"
   },
   "source": [
    "#### Tensors are an enhanced, uniform variant of multi-dimensional lists that torch operations can eat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1678897494402,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "gI8gFtZwGOHy",
    "outputId": "f1eabfef-47ca-4e16-d50b-ab4793ffd8c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occured in matmul(): argument 'input' (position 1) must be Tensor, not list\n"
     ]
    }
   ],
   "source": [
    "A = [[ 0.0, 1.0], [ 1.0 , 0.0]]\n",
    "\n",
    "try:\n",
    "  torch.matmul(A, A) # throws a TypeError\n",
    "except TypeError as error:\n",
    "  print(f\"An error occured in {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1678897494402,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "qkXDlE2WGfCY",
    "outputId": "a62effb6-2887-47b4-d0d5-cc4611c7f2d7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0.],\n",
       "        [0., 1.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_tensor = torch.tensor(A)\n",
    "\n",
    "torch.matmul(A_tensor, A_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o9veRCZCKlqp"
   },
   "source": [
    "Notice that nested lists that are candidates for tensors must be uniform in every dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1678897494402,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "pLXm_LvnLdGC",
    "outputId": "36a2aa7a-5160-4d1b-d128-c3df31cce635"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not form a tensor: expected sequence of length 3 at dim 2 (got 2)\n"
     ]
    }
   ],
   "source": [
    "B = [\n",
    "    [[1, 2, 3], [4, 5, 6]],\n",
    "    [[0, 0], [1, 1]]\n",
    "]\n",
    "\n",
    "\n",
    "try:\n",
    "  B_tensor = torch.tensor(B) # throws a ValueError\n",
    "except ValueError as error:\n",
    "  print(f\"Could not form a tensor: {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sLtbSji2NXIB"
   },
   "source": [
    "**Exercise.** Create a tensor `I_tensor`, which is a 3x3 identity matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1678897494402,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "M4a53MyvNnOZ"
   },
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "\n",
    "I = None \n",
    "\n",
    "I_tensor = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8N_bgQbmHt5x"
   },
   "source": [
    "#### Tensors have a multi-dimensional `size`, also known as `shape`\n",
    "The shape of a tensor describes the sizes of its individual tensor dimensions (also known as *axes*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1678897494403,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "xp9ZCybhH9Fl",
    "outputId": "70631a89-39df-4665-abbc-83be20cf8a9b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1678897494403,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "xNgPKohDIBZm",
    "outputId": "17d21461-eb9d-4da7-be31-2e19a4fb5d7b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_tensor.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x2rzIVf0IRDl"
   },
   "source": [
    "Notice that `A` consists of two lists containing two elements each.\n",
    "Correspondingly, `A_tensor` has size of `[2, 2]`, meaning that `A_tensor` consists of to sub-tensors, namely `A_tensor[0]` and `A_tensor[1]`, containing two elements each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1678897494403,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "c9UQIDJMI7IZ",
    "outputId": "0a6382de-0d8a-485c-b934-e3b68455f6d2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_tensor[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HqgN1NydJJfC"
   },
   "source": [
    "If we take `B` such that `B` contains two lists of lists, such as in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1678897494403,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "ZAJfXvqlIZra"
   },
   "outputs": [],
   "source": [
    "B = [\n",
    "    [[1, 2, 3], [4, 5, 6]],\n",
    "    [[0, 0, 0], [1, 1, 1]]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hd4A95w0JSHw"
   },
   "source": [
    "Then we can turn it into a `B_tensor` of size `[2,2,3]`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1678897494403,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "X6KrSZ3aJkzt",
    "outputId": "86183138-d9da-4d68-a029-f84b79a18e68"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B_tensor = torch.tensor(B)\n",
    "B_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6drfln9LJrRW"
   },
   "source": [
    "... meaning that `B_tensor` consits of two two-dimensional sub-tensors, `B_tensor[0]`, and `B_tensor[1]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H1IUoaVnNwL1"
   },
   "source": [
    "**Exercise.** What should be the shape of `I_tensor`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1678897494404,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "jBaWraIFN0xF"
   },
   "outputs": [],
   "source": [
    "I_tensor_shape_intended = torch.Size( [  ] ) # modify this line with your guess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-KzK5SbOMG2"
   },
   "source": [
    "**Exercise.** Retrieve the shape of `I_tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1678897494404,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "E9khgv9NORPi"
   },
   "outputs": [],
   "source": [
    "I_tensor_shape = None # modify this line with the correct code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PujSmcTuOXVE"
   },
   "source": [
    "**Exercise.** Are they the same?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1678897494404,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "kNlLXAAQOgj9",
    "outputId": "b1bba9eb-9556-4e29-e1c2-590d46270669"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I_tensor does not have shape as intended.\n"
     ]
    }
   ],
   "source": [
    "# Run this code block.\n",
    "\n",
    "if I_tensor_shape_intended == I_tensor_shape:\n",
    "  print(\"I_tensor has shape as intended.\")\n",
    "else:\n",
    "  print(\"I_tensor does not have shape as intended.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5QiVwQxKJ_R"
   },
   "source": [
    "#### You can access arbitrary sub-tensors of every tensor\n",
    "\n",
    "For this, you can use the python's usual slicing notation. For example, to get the second element of each of the deepest lists of `B` in the corresponding tensor, one can simply write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1678897494404,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "c1PnNsaaL_Ij",
    "outputId": "4992120d-c4d2-4494-bc6d-62aeb1d7a93a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 5],\n",
       "        [0, 1]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B_tensor[:,:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uaFKUKGsGnmC"
   },
   "source": [
    "#### Tensors can be either trainable or non-trainable\n",
    "\n",
    "The trainable tensors are the ones that have `require_gradient` set to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1678897494404,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "UZrpvNetHHks",
    "outputId": "e5d1ee05-e703-439a-f371-54dd4c11c2c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_tensor is trainable: False\n",
      "A_trainable is trainable: True\n"
     ]
    }
   ],
   "source": [
    "A_trainable = torch.tensor(A, requires_grad=True)\n",
    "\n",
    "print(f\"A_tensor is trainable: {A_tensor.requires_grad}\")\n",
    "print(f\"A_trainable is trainable: {A_trainable.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9CoaVWTtHjN1"
   },
   "source": [
    "#### Tensors can be used in computations element-wise, as long as the dimensions match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1678897494405,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "vdFQUCmzHijV",
    "outputId": "05798d8f-b39b-424f-cb0a-7c50b4999b59"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.7000,  5.4000, 11.1000],\n",
       "         [18.8000, 28.5000, 40.2000]],\n",
       "\n",
       "        [[ 0.0000,  0.0000,  0.0000],\n",
       "         [ 1.7000,  1.7000,  1.7000]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B_tensor + B_tensor ** 2 - 0.3 * B_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z5Ndj7QtUitM"
   },
   "source": [
    "**Exercise.** With the help of PyTorch documentation online, find the square root of $B^3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1678897494405,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "WmGpu-sjZrNQ"
   },
   "outputs": [],
   "source": [
    "# your solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0lKIe3UBGO_U"
   },
   "source": [
    "## Programs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rwUqmRDnwStr"
   },
   "source": [
    "You are certainly familiar with the notion of a classical computer *program*. For our purposes, a program $f$ is an information processing device that takes some inputs $x$ and produces outputs $f(x)$.\n",
    "\n",
    "Programs can be *pure*, meaning they have no side effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1678897494405,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "W0SHjFoVxZmr"
   },
   "outputs": [],
   "source": [
    "def is_large(x):\n",
    "  threshold = 10\n",
    "  if x > threshold:\n",
    "    return True\n",
    "  else:\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMJHOcUyxnn1"
   },
   "source": [
    "... or \"impure\", meaning that executing them alters some fixed memory state in the computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1678897494405,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "8yH6g2acx11M"
   },
   "outputs": [],
   "source": [
    "large_number_count = 0\n",
    "\n",
    "def impure_is_large(x):\n",
    "  threshold = 10\n",
    "  if x > threshold:\n",
    "    large_number_count += 1\n",
    "    return True\n",
    "  else:\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XRzZXpCFxvsK"
   },
   "source": [
    "Notice that the variable `threshold` in both `is_large` and `impure_is_large` does not really encode a state of the program, but is a parameter determining which numbers will and which numbers won't be considered \"large\".\n",
    "\n",
    "Throughout this session, we will only be dealing with pure programs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U6JZJgF0yR5o"
   },
   "source": [
    "## Differentiable Programs and Why They Are So Special\n",
    "\n",
    "The entire world is now interested in a particular sub-class of programs, called *differentiable programs*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11getKqlvbJE"
   },
   "source": [
    "A differentiable program is a program $f$ such that $f$ is differentiable with respect to its parameters. Here is an example of a differentiable program $f$ taking $x$ as input and multiplying it by a parameter $p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1678897494405,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "l0fCXWtzv6et"
   },
   "outputs": [],
   "source": [
    "p = torch.tensor([ 1.0 ], requires_grad=True)\n",
    "\n",
    "def f(x):\n",
    "  global p\n",
    "  return p * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AEV5JmM5v8IG"
   },
   "source": [
    "Apart from forcing software engineers to dust off their high-school calculus knowledge, what are these differentiable programs actually good for? Why has the entire software engineering and data science world gone crazy over them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MWwo3vb00uaO"
   },
   "source": [
    "We won't keep you in suspense, here's the \"secret\":\n",
    "\n",
    "> Given input-output data, differentiable programs can be taught, through trial and error, to use the right parameters.\n",
    "\n",
    "So, in the example of `f` above, we could train the program to learn the \"true\" value of `p`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mfHkDnBA1km6"
   },
   "source": [
    "The method enabling this is the one of gradient descent training. This has been covered well in many lectures and online resources. If you need a quick refresher, have a look through at the corresponding videos from the Computational Thinking course, available [here](https://)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S4gmO1GKilew"
   },
   "source": [
    "### Gradient Computation in PyTorch\n",
    "At the helm of gradient-descent training in PyTorch is the `autograd` module. `torch.autograd` is PyTorch's automatic differentiation engine that powers gradient-descent training.\n",
    "\n",
    "Suppose you take some trainable tensor $x$ and pass it through $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1336,
     "status": "ok",
     "timestamp": 1678897495732,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "ovOmObj-aCnr",
    "outputId": "cd694e2e-d643-4e05-a50b-390d9133fb10"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.2000], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([ 5.2 ], requires_grad=True)\n",
    "output = f(x)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QcDXcjV3aTK5"
   },
   "source": [
    "Notice that `output` now has an additional field, `grad_fn`, that was set by the `autograd` system to keep track of what operations have been performed on `x` to arrive at output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DKlQaJ9Ialbh"
   },
   "source": [
    "Now, given some expected output for $f(x)$, say $1$, `autograd` allows you to compute an indication of how `p` needs to be changed in order for $f(x)$ to eventually yield the correct outputs. This is done in a process called *backward pass*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 1336,
     "status": "ok",
     "timestamp": 1678897495733,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "V6nNrPt4bLbh"
   },
   "outputs": [],
   "source": [
    "expected_output = torch.tensor([1.0])\n",
    "loss = (output - expected_output) ** 2\n",
    "loss.backward(retain_graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M6I-wj_QbV83"
   },
   "source": [
    "This indication can then by inspected by asking `p` what its gradient is by reading `p.grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1678897495734,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "_YanreQybVVa",
    "outputId": "22b97fc2-6397-47bb-b779-e43b43d70832"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([43.6800])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2vNxdfQgcNcq"
   },
   "source": [
    "This indication can be interpreted as\n",
    "\n",
    "> Decreasing `p` by some small $\\epsilon$ will decrease the loss by $87.36\\epsilon$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVLVUzhpjQOS"
   },
   "source": [
    "And, as you already know, leveraging the negation of gradient as the indication of the direction in which one should modify the parameters in order to descent towards lower values of the loss, gradient descent training is simply the routine under which one iteratively computes and then applies the gradient of the loss function with respect to parameters of the computation to minimise the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s93DpbnznITx"
   },
   "source": [
    "**Exercise.** Fill in the code below to compute the gradients for `p` equal to `0.75`, `0.5`, `0.25`, and `0.20`. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1678897495734,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "ZoWVAgtCnlT6",
    "outputId": "73c9deae-1c8f-42da-86a5-7adab363eee5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For p = 0.75, the gradient is None\n",
      "For p = 0.50, the gradient is None\n",
      "For p = 0.25, the gradient is None\n",
      "For p = 0.20, the gradient is None\n"
     ]
    }
   ],
   "source": [
    "# case p = 0.75\n",
    "p = torch.tensor([ 0.75 ], requires_grad=True)\n",
    "\n",
    "#  - you want something gradienty here :)\n",
    "\n",
    "print(f\"For p = 0.75, the gradient is {p.grad}\")\n",
    "\n",
    "# case p = 0.50\n",
    "p = torch.tensor([ 0.50 ], requires_grad=True)\n",
    "\n",
    "#  - also here\n",
    "\n",
    "print(f\"For p = 0.50, the gradient is {p.grad}\")\n",
    "\n",
    "# case p = 0.25\n",
    "p = torch.tensor([ 0.25 ], requires_grad=True)\n",
    "\n",
    "#  - ...\n",
    "\n",
    "print(f\"For p = 0.25, the gradient is {p.grad}\")\n",
    "\n",
    "# case p = 0.20\n",
    "p = torch.tensor([ 0.20 ], requires_grad=True)\n",
    "\n",
    "print(f\"For p = 0.20, the gradient is {p.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "33PSV9YLnxy4"
   },
   "source": [
    "### A Basic Training Loop\n",
    "As hinted on by the above example, modifying the parameters of a differentiable program in the direction opposite to its gradient (i.e. in the direction in which the loss decreases most rapidly) generally guides the differentiable programs towards a minimum in the loss.\n",
    "\n",
    "This process can be repeated iteratively, to form what is called a *training loop*. A typical training procedure of a differentiable program looks as follows:\n",
    "\n",
    "\n",
    "1. Initialise the parameters of the differentiable program according to an appropriate scheme.\n",
    "2. Take the inputs provided and perform a *forward pass* -- apply the program to the inputs.\n",
    "3. Compute the loss between the expected outputs and the actual outputs of the program.\n",
    "4. Compute the gradient of the loss with respect to the program's parameters.\n",
    "5. Scale the gradients by the desired pace of descent -- *the learning rate** -- and update the parameters accordingly.\n",
    "6. If not done yet, go back to 2..\n",
    "\n",
    "\n",
    "You already possess all the basic ingredients necessary to implement such a training procedure yourself. Let's do that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_nYngzhdoCRt"
   },
   "source": [
    "**Exercise.** Fill in the code below to arrive at a working implementation of a gradient descent training loop for $f$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 131
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "error",
     "timestamp": 1678897495734,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "dn1YONH9lr5r",
    "outputId": "6fd49534-83d3-4d06-af96-582bee2eac6c"
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-32-90403f8afc50>\"\u001b[0;36m, line \u001b[0;32m20\u001b[0m\n\u001b[0;31m    p.grad.data.zero_()\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def train_f(x, y, learning_rate: float = 0.01, number_of_iterations: int = 50):\n",
    "  global p\n",
    "  # TODO: initialise p to a random tensor between 0 and 1 (hint: use torch.rand)\n",
    "  # remember that you need p to be trainable!\n",
    "\n",
    "  for iteration in range(1, number_of_iterations+1):\n",
    "    # TODO: perform a \"forward pass\" (apply f to x)\n",
    "    output = None\n",
    "    \n",
    "    # compute the loss\n",
    "    loss = torch.sum((output - y) ** 2) / output.size(0)\n",
    "\n",
    "    # TODO: compute the gradient of `loss` given p\n",
    "\n",
    "    # TODO: subtract learning_rate*(gradient of p) from p ...\n",
    "    with torch.no_grad():\n",
    "      # ... here\n",
    "\n",
    "    # finally, erase the gradients for the next iteration\n",
    "    p.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4mmw4UKFl5Kh"
   },
   "source": [
    "**Exercise.** Choose a value of `p_true` -- the parameter value for $f$ to be learned. Then, run the code below to check the correctness of your training loop from above. If you struggle to get the right answer, consider decreasing the learning rate and increasing the number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "aborted",
     "timestamp": 1678897495734,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "9xA81E0Ll1N5"
   },
   "outputs": [],
   "source": [
    "p_true = None # the parameter value of p to be learned\n",
    "datapoint_count = None # the number of datapoints to use for training in every iteration of `train_f`\n",
    "\n",
    "x = torch.rand((datapoint_count,))\n",
    "y = x * p_true\n",
    "\n",
    "train_f(x, y)\n",
    "print(f\"The true value is {p_true.item()}, the value learned by gradient descent is {p.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3PXg-iPLsNLV"
   },
   "source": [
    "## Introducing Neural Networks\n",
    "Neural networks are a particular class of differentiable programs, for which it has been theoretically proven that they can learn to approximate an arbitrary integrable function arbitrarily well, as long as they are given enough *representational power*.\n",
    "\n",
    "*Here is where the deep learning black magic begins.* \n",
    "\n",
    "Classical programs consist of a sequence of specific operations such as addition or conditional value assignment. Neural networks are differentiable programs that consist of a sequence of amenable elementary building blocks, traditionally referred to as *layers*, that can ultimately perform a wide variety of operations. The \"bigger\" these layers are, the more complex behaviour they can learn to exhibit.\n",
    "\n",
    "There exists several popular types of neural network building blocks, including the trainable *linear layer*, or the non-trainable *activation*, *softmax*, and *dropout layers*, to name but a few. The combination of a linear layer and an activation layer is sometimes referred to as *dense layer* and is the basic building block of a *deep neural network*.\n",
    "\n",
    "The amount of *representational power* network has is determined by the sizes of its trainable layers. Linear layers have a \"width\" (the number of constituent neurons). The wider the layer, the more fine-grained operation it is capable of representing. Whether it can learn to represent this operation is, however, an entirely different question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSgEwtUzQSEq"
   },
   "source": [
    "### Constructing Neural Networks\n",
    "Without further ado, let use these building blocks to form neural networks.\n",
    "\n",
    "Knowing the format of the operation of individual layers, you could go ahead and implement them directly. To avoid uncanny detail, we will instead use the ready-made implementations of these layers from the `torch.nn` module.\n",
    "\n",
    "In general, the [documentation](https://pytorch.org/docs/stable/nn.html) of the `torch.nn` module is what you want to turn to to understand a new layer type.\n",
    "\n",
    "We walk you through creating instances of various layer types in the code below. We directly use the instances to operate input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CX5xxzB1vXwi"
   },
   "source": [
    "#### Linear Layers\n",
    "\n",
    "Let us begin with the most basic layer in deep learning, the Linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "aborted",
     "timestamp": 1678897495734,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "9dUDE6jjQRQW"
   },
   "outputs": [],
   "source": [
    "# construct a linear layer that takes a tensor of size (3,) and produces a \n",
    "#  tensor of size (5,)\n",
    "linear_layer = torch.nn.Linear(3, 5)\n",
    "\n",
    "# pass [1, 2, 3] through the layer\n",
    "example_input = torch.tensor([ 1, 2, 3 ], dtype=torch.float)\n",
    "linear_layer(example_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sqrHdPdHvehw"
   },
   "source": [
    "Notice that as promised in the call to `nn.Linear(3, 5)`, the output tensor has 5 entries. Its output values are the result of an internal state (the layer *weights*) that has been initialised at random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3SQI0UMFwxj2"
   },
   "source": [
    "#### Activation Layers\n",
    "\n",
    "Several types of activation layers exist, most notably the logistic sigmoid, rectified linear unit (ReLU), and the hyperbolic tangent. Each of these has a layer in `torch.nn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "aborted",
     "timestamp": 1678897495734,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "J7tTJxVGu1wS"
   },
   "outputs": [],
   "source": [
    "# construct a ReLU layer\n",
    "relu_layer = torch.nn.ReLU()\n",
    "\n",
    "# pass [ -3, -2, -1, 0, +1, +2, +3 ] through the ReLU layer\n",
    "example_input = torch.tensor([ -3, -2, -1, 0, +1, +2, +3 ], dtype=torch.float)\n",
    "relu_layer(example_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "aborted",
     "timestamp": 1678897495735,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "st3FTSinzry1"
   },
   "outputs": [],
   "source": [
    "# construct a sigmoid layer\n",
    "sigmoid_layer = torch.nn.Sigmoid()\n",
    "\n",
    "# pass [ -3, -2, -1, 0, +1, +2, +3 ] through the sigmoid layer\n",
    "sigmoid_layer(example_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "aborted",
     "timestamp": 1678897495735,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "VHPmBuUdz4xL"
   },
   "outputs": [],
   "source": [
    "# construct a tanh layer\n",
    "tanh_layer = torch.nn.Tanh()\n",
    "\n",
    "# pass [ -3, -2, -1, 0, +1, +2, +3 ] through the tanh layer\n",
    "tanh_layer(example_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvrPbIhrz7J1"
   },
   "source": [
    "As you can see, going from minus infinity towards infinity around 0, the ReLU transits from constant 0 to linear behaviour at 0, the logistic sigmoid proceeds to climb from 0 towards 1, and tanh climbs from -1 towards +1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qLC-6JYK0Oji"
   },
   "source": [
    "#### Dropout Layers\n",
    "\n",
    "It is sometimes to the advantage of model training to \"drop out\" some of the incoming values at random. To this end, `torch.nn` provides the `Dropout` layer, which can be parametrised at construction with the probability of an input value being dropped out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "aborted",
     "timestamp": 1678897495735,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "8PLHSgNbz54s"
   },
   "outputs": [],
   "source": [
    "# construct a dropout layer with probability 0.0\n",
    "dropout_layer = torch.nn.Dropout(p=0.5)\n",
    "\n",
    "# pass [ -3, -2, -1, 0, +1, +2, +3 ] through the dropout layer\n",
    "dropout_layer(example_input)\n",
    "\n",
    "# with p=0.5, roughly half of the inputs should be dropped out on average, \n",
    "#  and the remaining outputs are scaled up by 1/(1-p) == 2 \n",
    "# run this snippet multiple times to observe the effects of random dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uri42H-24L80"
   },
   "source": [
    "#### Softmax\n",
    "Sometimes we wish to interpret an $n$-dimensional vector of real values as scores in favour of a single one of $n$ discrete elements possessing a certain property. To this end, we often use the \"softmax\" layer.\n",
    "\n",
    "The softmax layer takes the $n$-dimensional vector of real values and produces an $n$-dimensional vector of values between $0$ and $1$, whose individual entries sum up to $1$.\n",
    "\n",
    "The bigger an entry of the input vector is relative to other entries, the closer its corresponding value in the output vector is to $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "aborted",
     "timestamp": 1678897495735,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "Lv4tVwYK3WZc"
   },
   "outputs": [],
   "source": [
    "# construct a softmax layer\n",
    "softmax_layer = torch.nn.Softmax(dim=0)\n",
    "\n",
    "# pass [ -3, -2, -1, 0, +1, +2, +3 ] through the softmax layer\n",
    "softmax_layer(example_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "aborted",
     "timestamp": 1678897495735,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "rvmT75wPoiYq"
   },
   "outputs": [],
   "source": [
    "# pass [ -1, 2, 5, 100 ] through the softmax layer\n",
    "softmax_layer(torch.tensor([ -1, 2, 5, 100 ], dtype=torch.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3hF_z6JC4M_V"
   },
   "source": [
    "### Putting the Layers Together -- Implementing a DNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CW_XVRYLuLJG"
   },
   "source": [
    "Neural networks are graphs of layers. In PyTorch, we generally tend to implement our neural networks as classes whose constructors construct the constituent parts of the network, and whose `forward` function passes the data through these parts.\n",
    "\n",
    "Below is an example implementation of a shallow neural network. This network is *shallow* as it contains only one hidden trainable layer (=layer that is not an input or output layer).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "aborted",
     "timestamp": 1678897495735,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "8boLlg2HwJhg"
   },
   "outputs": [],
   "source": [
    "class ShallowNeuralNet(nn.Module):\n",
    "    def __init__(self, input_width: int, hidden_layer_width: int, output_width):\n",
    "        super().__init__()\n",
    "        self.hidden_layer = nn.Linear(input_width, hidden_layer_width)\n",
    "        self.hidden_relu = nn.ReLU()\n",
    "        self.output_layer = nn.Linear(hidden_layer_width, output_width)\n",
    "\n",
    "    def forward(self, input):\n",
    "        hidden_trainable_output = self.hidden_layer(input)\n",
    "        hidden_relu_output = self.hidden_relu(hidden_trainable_output)\n",
    "        output = self.output_layer(hidden_relu_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Le9mkI0Lwz0i"
   },
   "source": [
    "Once the network behaviour has been described in this fashion, we can create an instance of the entire network at once and use it to process data in exactly the same way as we would use layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "aborted",
     "timestamp": 1678897495736,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "jjQx_ztbwzGG"
   },
   "outputs": [],
   "source": [
    "shallow_nn_instance = ShallowNeuralNet(5, 10, 2)\n",
    "\n",
    "example_input = torch.ones(5)\n",
    "shallow_nn_instance(example_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AxG85zD10uCD"
   },
   "source": [
    "**Exercise.** Fill in the snippet below to arrive at an implementation of a deep ReLU neural networks with layer profile given by a list of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "aborted",
     "timestamp": 1678897495737,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "GIP3GTxM0tZF"
   },
   "outputs": [],
   "source": [
    "class DeepNeuralNet(nn.Module):\n",
    "  def __init__(self, input_width, hidden_layer_profile, output_width, output_activation=None):\n",
    "    super().__init__()\n",
    "    self.layers = nn.ModuleList()\n",
    "\n",
    "    # create the first hidden layer\n",
    "    self.layers.append(nn.Linear(input_width, hidden_layer_profile[0]))\n",
    "    self.layers.append(nn.ReLU())\n",
    "\n",
    "    # create the internal hidden layers\n",
    "    for in_width, out_width in zip(hidden_layer_profile[0:-1], hidden_layer_profile[1:]):\n",
    "      self.layers.append(nn.Linear(in_width, out_width))\n",
    "      self.layers.append(nn.ReLU())\n",
    "\n",
    "    # create the output layer\n",
    "    self.output_layer = nn.Linear(hidden_layer_profile[-1], output_width)\n",
    "    self.output_activation = nn.Identity() if not output_activation else output_activation\n",
    "  \n",
    "  def forward(self, input):\n",
    "    x = input\n",
    "\n",
    "    # loop through the layers to produce the output of the hidden network\n",
    "    for layer in self.layers:\n",
    "      # TODO: pass the intermediate output of the previous layer through the current layer\n",
    "      x = None\n",
    "\n",
    "    # TODO: produce the output of the network from the intermediate output of the last hidden layer\n",
    "    output_before_activation = None\n",
    "\n",
    "    # TODO: engage the optional activation in self.output_activation on the output_before_activation\n",
    "    output = None\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ctV1s9_2lw_"
   },
   "source": [
    "**Exercise.** Test the class for generic deep neural networks below. Does everything work as expected?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "aborted",
     "timestamp": 1678897495737,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "ApoMITVs2wLd"
   },
   "outputs": [],
   "source": [
    "# try passing a tensor of random numbers through the network\n",
    "input1 = torch.rand((10,))\n",
    "deep_nn_instance = DeepNeuralNet(input_width=10, hidden_layer_profile=[10, 7, 5], output_width=1)\n",
    "\n",
    "deep_nn_instance(input1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NOafensI3MNS"
   },
   "source": [
    "A typical neural network design pattern appearing in search and recommender systems is that of \"two towers\". Explained in brief, the network consists of two separate sub-networks, one for queries and one for results. In some special cases when the modalities of the queries and results are the same, the towers can be made to \"share weights\" (use the same architecture and parameters to process their respective inputs). In such case, one might talk of the \"twin tower\" architecture being used.\n",
    "\n",
    "**Exercise (Weight Sharing).** Using the class `DeepNeuralNet` you implemented above, fill in the code below to produce an implementation of the twin tower architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "aborted",
     "timestamp": 1678897495737,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "49VQyDDT5I6m"
   },
   "outputs": [],
   "source": [
    "class TwinDeepNeuralNet(nn.Module):\n",
    "  def __init__(self, input_width, hidden_layer_profile, output_width):\n",
    "    super().__init__()\n",
    "    \n",
    "    # TODO: use DeepNeuralNet to construct a network that can perform the function of a \"twin tower\" network\n",
    "\n",
    "  def forward(self, input):\n",
    "    # identify the query and the value as sub-tensors of the input tensor\n",
    "    input_query = input[0,:]\n",
    "    input_value = input[1,:]\n",
    "\n",
    "    # TODO: use the layer(s) or sub-network(s) initialised in the constructor to implement the functionality of a \"twin tower\" network\n",
    "    output_query = None\n",
    "    output_value = None\n",
    "\n",
    "\n",
    "    # form a single output tensor as a disjoint union of the query and value tensors\n",
    "    output_query = output_query.unsqueeze(0)\n",
    "    output_value = output_value.unsqueeze(0)\n",
    "    output = torch.cat([output_query, output_value], dim=0)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zaiTPH-i9etC"
   },
   "source": [
    "On the simple example below, test whether your implementation works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "aborted",
     "timestamp": 1678897495737,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "Yx19kOVi9hWG"
   },
   "outputs": [],
   "source": [
    "testing_input = torch.ones((2, 5))\n",
    "\n",
    "twin_nn_instance = TwinDeepNeuralNet(5, [10, 10], 1)\n",
    "testing_output = twin_nn_instance(testing_input)\n",
    "\n",
    "equality = torch.all(testing_output[0] == testing_output[1])\n",
    "\n",
    "print(f\"The outputs of the twins are {'equal' if equality else 'not equal'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HqrY3Z77-TYq"
   },
   "source": [
    "### A Training Loop for Neural Networks\n",
    "In a previous section concerning differentiable programs, we introduced the intuition for using the gradient information due to a choice of loss function to find optimal parameters of a differentiable program. \n",
    "\n",
    "This is exactly what we do for neural networks as well in order to train them to have the behaviour we desire of them.\n",
    "\n",
    "We give code for optimisation of a neural network `net` with particular loss function `loss` on dataset loaded by a `dataloader` below, and comment on it step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "aborted",
     "timestamp": 1678897495738,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "0GWPK-epiM3Q"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def training_loop(dataloader, net, loss_fn, optimiser, verbosity=3):\n",
    "    size = len(dataloader.dataset)\n",
    "    last_print_point = 0\n",
    "    current = 0\n",
    "\n",
    "    acc_loss = 0\n",
    "    acc_count = 0\n",
    "\n",
    "    # for every slice (X, y) of the training dataset\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # perform a forward pass to compute the outputs of the net\n",
    "        pred = net(X)\n",
    "\n",
    "        # calculate the loss between the outputs of the net and the desired outputs\n",
    "        loss_val = loss_fn(pred, y)\n",
    "        acc_loss += loss_val.item()\n",
    "        acc_count += 1\n",
    "\n",
    "        # zero the gradients computed in the previous step \n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        # calculate the gradients of the parameters of the net\n",
    "        loss_val.backward()\n",
    "\n",
    "        # use the gradients to update the weights of the network\n",
    "        optimiser.step()\n",
    "\n",
    "        # compute how many datapoints have already been used for training\n",
    "        current = batch * len(X)\n",
    "\n",
    "        # report on the training progress roughly every 10% of the progress\n",
    "        if verbosity >= 3 and (current - last_print_point) / size >= 0.1:\n",
    "            loss_val = loss_val.item()\n",
    "            last_print_point = current\n",
    "            print(f\"loss: {loss_val:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    return acc_loss / acc_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o84sL9H2pNR2"
   },
   "source": [
    "We now possess all the tools necessary for constructing simple neural networks and for training them towards some particular behaviour by gradient descent loss minimisation. Let us put these tools to good use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ogK_KsxYZDN"
   },
   "source": [
    "### Learning a Boolean Function\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MAPqtEvW3Kzj"
   },
   "source": [
    "Let us consider the problem of learning a random Boolean function $f: \\left\\{ 0,1 \\right\\}^n \\to \\left\\{ 0,1 \\right\\}^n$. It might sound a bit dry at first, but bear with us, it is a very natural and tractable example for the examination of the representational power of various types of neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "aborted",
     "timestamp": 1678897495738,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "CVoL-jQFYnDi"
   },
   "outputs": [],
   "source": [
    "def make_binary_array(number: int, length: int) -> list:\n",
    "\treturn [ (number>>k)&1 for k in range(0, length) ]\n",
    "\n",
    "data_x_list = []\n",
    "\n",
    "for i in range(0, 256):\n",
    "  data_x_list.append(make_binary_array(i, 8))\n",
    "\n",
    "data_x = torch.tensor(data_x_list, dtype=torch.float)\n",
    "data_y = torch.randint(low=0, high=2, size=(256, 8)).type(torch.float)\n",
    "dataset = torch.utils.data.TensorDataset(data_x, data_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5FqdC15d3gkg"
   },
   "source": [
    "The above code generates a dataset of $(x, y)$ pairs where $x$ is any $n=8$-bit signal and $y = f(x)$, with $f$ chosen uniformly at random. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZTVEVnlMBrCt"
   },
   "source": [
    "Given these examples, can we learn a neural network that performs the function of $f$? Yes! Just run the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "aborted",
     "timestamp": 1678897495738,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "J-FDh-msFG7X"
   },
   "outputs": [],
   "source": [
    "net = DeepNeuralNet(input_width=8, hidden_layer_profile=[256], output_width=8, output_activation=nn.Sigmoid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "aborted",
     "timestamp": 1678897495738,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "JAUZSwWBLyd4"
   },
   "outputs": [],
   "source": [
    "def testing_loop(dataloader, net):\n",
    "  size = len(dataloader.dataset)\n",
    "  last_print_point = 0\n",
    "  current = 0\n",
    "\n",
    "  acc_correct = 0\n",
    "  acc_count = 0\n",
    "\n",
    "  # for every slice (X, y) of the training dataset\n",
    "  with torch.no_grad():\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        # perform a forward pass to compute the outputs of the net\n",
    "        pred = net(X)\n",
    "\n",
    "        # round the predictions (0 - 0.5 towards zero, >0.5 towards one)\n",
    "        pred_rounded = torch.round(pred)\n",
    "\n",
    "        # compute the number of correct entries\n",
    "        acc_correct += torch.count_nonzero(pred_rounded == y).item()\n",
    "        acc_count += y.numel()\n",
    "\n",
    "  return acc_correct / acc_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "aborted",
     "timestamp": 1678897495738,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "FgKbUBdWMAm1"
   },
   "outputs": [],
   "source": [
    "def train(dataloader, net, loss_fn, optimiser, epochs, verbosity=3):\n",
    "  least_loss = None\n",
    "\n",
    "  for t in range(epochs):\n",
    "    if verbosity >= 3:\n",
    "      print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    \n",
    "    mean_loss = training_loop(dataloader, net, loss_fn, optimiser, verbosity=verbosity)\n",
    "    accuracy = testing_loop(dataloader, net)\n",
    "    if not least_loss or mean_loss < least_loss:\n",
    "      least_loss = mean_loss\n",
    "    if verbosity >= 2:\n",
    "      print((f\"Epoch {t+1}: \" if verbosity >= 3 else \"\") + f\"mean loss {mean_loss}, validation accuracy {accuracy:.2%}\")\n",
    "    if verbosity >= 3:\n",
    "      print(\"\\n\")\n",
    "  \n",
    "  if verbosity >= 1:\n",
    "    print(f\"Training complete, least loss {least_loss}, final validation accuracy {accuracy:.2%}\")\n",
    "  return least_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "aborted",
     "timestamp": 1678897495738,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "Qv36ZZ4aXkmA"
   },
   "outputs": [],
   "source": [
    "training_dataloader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "loss_fn = nn.BCELoss()\n",
    "optimiser = torch.optim.Adam(net.parameters(), lr=5e-3)\n",
    "\n",
    "least_loss = train(training_dataloader, net, loss_fn, optimiser, epochs=250, verbosity=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kkVWsgZU36Bg"
   },
   "source": [
    "Okay, this is encouraging. We are getting a loss in the order of $10^{-3}$ (which is relatively little for mean binary cross-entropy) and 100% accuracy. You will notice that as the training progresses, the loss tends to decrease and the accuracy increases. You will also notice that our neural network has only one hidden layer of 256 neurons. But are all those neurons really necessary?\n",
    "\n",
    "Let's push things to an extremum and consider a network that has exactly one neuron in its hidden layer. In other words, all of the information about the input the output neurons have must be contained in exactly one activated number, and the hidden layer of such a network is an information bottleneck. All other parameters constant, what sort of loss values and accuracies will we be getting under such circumstances?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "aborted",
     "timestamp": 1678897495739,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "_nIrvnj98dN5"
   },
   "outputs": [],
   "source": [
    "slender_net = DeepNeuralNet(input_width=8, hidden_layer_profile=[ 1 ], output_width=8, output_activation=nn.Sigmoid())\n",
    "\n",
    "training_dataloader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "loss_fn = nn.BCELoss()\n",
    "optimiser = torch.optim.Adam(slender_net.parameters(), lr=5e-3)\n",
    "\n",
    "least_slender_loss = train(training_dataloader, slender_net, loss_fn, optimiser, epochs=250, verbosity=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zFLICSAFATyE"
   },
   "source": [
    "We see that with one hidden neuron, we learn to predict the values of $f(x)$ only marginally better than a coin flip, and that this corresponds to a relatively large value of the binary cross-entropy loss.\n",
    "\n",
    "Okay. So there is a number of hidden neurons (256) that is sufficient for learning $f$ with 100% accuracy, and there is a number of hidden neurons (1) that is clearly insufficient to learn anything but some rough indication of the correct output. \n",
    "\n",
    "*   With one hidden neuron, we have starved the network of representational power to the extent that it is only slightly better than tossing a fair coin at predicting $f$.\n",
    "*   With 256 hidden neurons, we have given the network enough representational power to learn $f$. Perhaps even too much.\n",
    "\n",
    "What happens in between these two extrema? And, is there a point - a number of neurons - beyond which the network fails to learn $f$ correctly but for which $f$ can still be learned?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o0wKjLNG4Yrn"
   },
   "source": [
    "**Exercise.** Find the least number of neurons $w_1$ such that the training of a shallow neural network with $w$ hidden neurons can still learn to execute $f$ with loss of at most $0.001$. Remember that you can adjust the learning rate and the number of epochs to get finer and more resource-efficient training. You can also set `verbosity=1` to avoid long listings of losses, though verbosity of above `1` might help with the investigation of whether the training losses plateau out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "aborted",
     "timestamp": 1678897495739,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "ATMk4IcY4vqb"
   },
   "outputs": [],
   "source": [
    "w = None\n",
    "net = DeepNeuralNet(input_width=8, hidden_layer_profile=[ w_1 ], output_width=8, output_activation=nn.Sigmoid())\n",
    "\n",
    "training_dataloader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "loss_fn = nn.BCELoss()\n",
    "optimiser = torch.optim.Adam(net.parameters(), lr=5e-3)\n",
    "least_loss = train(training_dataloader, net, loss_fn, optimiser, epochs=250, verbosity=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j2XD3gXi5GdP"
   },
   "source": [
    "**Exercise.** It has been theoretically proven that deep neural networks (that is, networks with more than one hidden layer) can learn the same functions as shallow neural nets while using comparatively fewer neurons and trainable weights. Can you find a `w_2`, a minimal number of neurons sufficient to learn the function $f$ with loss of at most $0.001$, such that the $w_2$ neurons can be distributed in multiple hidden layers? The number of layers you end up using is up to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "aborted",
     "timestamp": 1678897495739,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "uT-6dQcE6UQw"
   },
   "outputs": [],
   "source": [
    "net = DeepNeuralNet(input_width=8, hidden_layer_profile=[ None, None, None, ], output_width=8, output_activation=nn.Sigmoid())\n",
    "\n",
    "training_dataloader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "loss_fn = nn.BCELoss()\n",
    "optimiser = torch.optim.Adam(net.parameters(), lr=5e-3)\n",
    "least_loss = train(training_dataloader, net, loss_fn, optimiser, epochs=250, verbosity=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DtlSN3BgDObv"
   },
   "source": [
    "### Section Takeaway\n",
    "\n",
    "We have seen that neural networks can be constructed as directed graphs of more elementary building blocks (shallow and deep nets) and other networks (two-tower nets).\n",
    "\n",
    "We have also introduced the training loop for a neural network that uses much of PyTorch machinery to perform gradient descent.\n",
    "\n",
    "We have used the above to train networks that learn a Boolean function. We observed that not all networks can learn all functions, and that the number of neurons and their arrangement (or, more precisely, trainable weights and their role within the network) influence the ability of a network to learn to approximate a function. Experimenting, we got the intuitive feel of the notion of *representational power*.\n",
    "\n",
    "Using all of that has been learned, we can now go and train networks that are perhaps more suitable for real-world applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CgPbCBVh-X4A"
   },
   "source": [
    "## Image Classification with DNNs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-XRALwj3OQO"
   },
   "source": [
    "Most machine learning workflows involve working with data, creating models, optimizing model parameters, and saving the trained models. This section introduces you to a complete ML workflow implemented in PyTorch, with links to learn more about each of these concepts.\n",
    "\n",
    "We will use the FashionMNIST dataset to train a neural network that predicts if an input image belongs to one of the following classes: T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, or Ankle boot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3QE2rLR_3UYN"
   },
   "source": [
    "### Working with Data\n",
    "\n",
    "PyTorch has two primitives to work with data: `torch.utils.data.DataLoader` and `torch.utils.data.Dataset`. `Dataset` stores the samples and their corresponding labels, and `DataLoader` wraps an iterable around the `Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "aborted",
     "timestamp": 1678897495739,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "UrJOdfWg3fyO"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import Compose, ToTensor, Lambda\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mXhX0V6x3fOv"
   },
   "source": [
    "PyTorch offers domain-specific libraries such as TorchText, TorchVision, and TorchAudio, all of which include datasets. For this tutorial, we will be using a TorchVision dataset.\n",
    "\n",
    "The `torchvision.datasets` module contains `Dataset` objects for many real-world vision data like CIFAR, COCO. In this tutorial, we use the `FashionMNIST` dataset. Every TorchVision `Dataset` includes two arguments: `transform` and `target_transform`, to modify the samples and labels respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "aborted",
     "timestamp": 1678897495739,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "XlvqVUin3sJK"
   },
   "outputs": [],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=Compose([\n",
    "      ToTensor(),\n",
    "      Lambda(lambda x: torch.flatten(x, start_dim=0))\n",
    "    ]),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=Compose([\n",
    "      ToTensor(),\n",
    "      Lambda(lambda x: torch.flatten(x, start_dim=0))\n",
    "    ]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1oPKr9xq30BL"
   },
   "source": [
    "We pass the `Dataset` as an argument to `DataLoader`. This wraps an iterable over our dataset, and supports automatic batching, sampling, shuffling and multiprocess data loading. Here we define a batch size of 64, i.e. each element in the dataloader iterable will return a batch of 64 features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "aborted",
     "timestamp": 1678897495739,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "CJWNfm3t4wDC"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TcuLF_FcYtQS"
   },
   "source": [
    "We can also have a quick peek at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "aborted",
     "timestamp": 1678897495739,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "klZSCJp2YsdR"
   },
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_dataloader))\n",
    "print('Shape of input tensor:', list(images.shape))\n",
    "ii = torch.reshape(images[0],(28,28))\n",
    "plt.imshow(ii, cmap='gray')\n",
    "plt.show()\n",
    "print('Label: ', int(labels[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q1sUIuMb37Is"
   },
   "source": [
    "### Creating Models\n",
    "As we have in previous sections when learning Boolean functions, to define a neural network in PyTorch we create a class that inherits from `nn.Module`. We define the layers of the network in the `__init__` function (the constructor) and specify how data will pass through the network in the forward function. To accelerate operations in the neural network, we move it to the GPU if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "aborted",
     "timestamp": 1678897495740,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "VMpr9W3b34sp"
   },
   "outputs": [],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "net = DeepNeuralNet(input_width=28 * 28, hidden_layer_profile=[512, 512], output_width=10).to(device)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OuUe4bYz4sKm"
   },
   "source": [
    "### Optimising Model Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5KVrwn14vwB"
   },
   "source": [
    "As illustrated before, to train a model, we need a loss function and an optimiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "aborted",
     "timestamp": 1678897495740,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "sDYCYpIM4t2n"
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.SGD(net.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ks-kMVbH45MH"
   },
   "source": [
    "The training loop from above will serve us well even in our current tasks. We also check the model’s performance against the test dataset to ensure it is learning -- in order to do so, we re-define the `testing_loop` function we used to learn Boolean functions in the new context of image classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "aborted",
     "timestamp": 1678897495740,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "o_HRTMXS42fX"
   },
   "outputs": [],
   "source": [
    "def testing_loop(dataloader, net,):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = net(X)\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    \n",
    "    return correct / size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "riWKYtev5MGt"
   },
   "source": [
    "The training process is conducted over several iterations (epochs). During each epoch, the model learns parameters to make better predictions. We print the model's accuracy and loss at each epoch; we would like to see the accuracy increase and the loss decrease with every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "aborted",
     "timestamp": 1678897495740,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "Sdns1QKu5LK4"
   },
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    training_loop(train_dataloader, net, loss_fn, optimiser)\n",
    "    validation_accuracy = testing_loop(train_dataloader, net)\n",
    "    print(f\"Validation Accuracy: {validation_accuracy:.2%}\\n\")\n",
    "print(\"Training Done!\")\n",
    "\n",
    "testing_accuracy = testing_loop(test_dataloader, net)\n",
    "print(f\"\\nTest Accuracy: {testing_accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ErIgSMFqLw_p"
   },
   "source": [
    "**Exercise.** Play around with different learning setups. Modifying just the learning rate and the number of epochs, how high can you take the validation accuracy? While doing so, do you also observe similar improvements in the test accuracy? (Remember that you can adjust the verbosity level not to have to read through long listings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "aborted",
     "timestamp": 1678897495740,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "IlxBrCs6L3tb"
   },
   "outputs": [],
   "source": [
    "train(train_dataloader, net, loss_fn, optimiser, epochs, verbosity=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hdz2qeSpQll8"
   },
   "source": [
    "**Exercise.** Play around with the architecture of the neural network that you train. Adding more layers and more neurons, can you take the test performance even higher than in the previous exercise?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "aborted",
     "timestamp": 1678897495740,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "lZbTBJlKRJQu"
   },
   "outputs": [],
   "source": [
    "# define the architecture of your neural network\n",
    "best_net = DeepNeuralNet(input_width=28 * 28, hidden_layer_profile=[ None ], output_width=10).to(device)\n",
    "print(best_net)\n",
    "\n",
    "# train it\n",
    "train(train_dataloader, net, loss_fn, optimiser, epochs, verbosity=3)\n",
    "\n",
    "# test it\n",
    "testing_accuracy = testing_loop(test_dataloader, net)\n",
    "print(f\"\\nTest Accuracy: {testing_accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LPpFsu8F59xI"
   },
   "source": [
    "### Saving and Loading Models\n",
    "Quite often you want to save your model, either to be later deployed in practice (on a website or in a mobile device, for example), or to be able to evaluate it later, in a different workflow. A common way to save a model is to serialise the internal state dictionary (containing the model parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "aborted",
     "timestamp": 1678897495740,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "iH2lJVkk5ROQ"
   },
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_aei7Jih6FyU"
   },
   "source": [
    "The process for loading a model includes re-creating the model structure and loading the state dictionary into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "aborted",
     "timestamp": 1678897495740,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "Pa519J6y6JR8"
   },
   "outputs": [],
   "source": [
    "model = DeepNeuralNet(28 * 28, [512, 512], 10)\n",
    "model.load_state_dict(torch.load(\"model.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aMSKxvms6ROa"
   },
   "source": [
    "This model can now be used to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "aborted",
     "timestamp": 1678897495740,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "sQFBDr7V6QOm"
   },
   "outputs": [],
   "source": [
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "x, y = test_data[0][0], test_data[0][1]\n",
    "with torch.no_grad():\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZhE7e1KYa15l"
   },
   "source": [
    "### MNIST from Scratch\n",
    "\n",
    "You have now seen the entire pipeline, going from the exploration of training data to the model evaluation.\n",
    "\n",
    "The final task for today is to use your new knowledge to get a running model that can classify MNIST dataset digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "aborted",
     "timestamp": 1678897495740,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "C21OVbIaclaJ"
   },
   "outputs": [],
   "source": [
    "train_data = datasets.MNIST(\n",
    "    root='data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=Compose([\n",
    "      ToTensor(),\n",
    "      Lambda(lambda x: torch.flatten(x, start_dim=0))\n",
    "    ]),\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root='data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=Compose([\n",
    "      ToTensor(),\n",
    "      Lambda(lambda x: torch.flatten(x, start_dim=0))\n",
    "    ]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "aborted",
     "timestamp": 1678897495741,
     "user": {
      "displayName": "Distributed Computing",
      "userId": "05493893522675058762"
     },
     "user_tz": -60
    },
    "id": "5ek1dsFqhPt5"
   },
   "outputs": [],
   "source": [
    "# Hint: you can follow and re-use the above code step-by-step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VG30Hk6WhUjb"
   },
   "source": [
    "Once you have a working instance, try varying the learning rate, batch size, and the sizes and numbers of individual layers in your network in order to get the best possible training accuracy.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "h5QiVwQxKJ_R",
    "uaFKUKGsGnmC",
    "9CoaVWTtHjN1",
    "CX5xxzB1vXwi",
    "qLC-6JYK0Oji",
    "uri42H-24L80",
    "DtlSN3BgDObv",
    "LPpFsu8F59xI"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
